diff --git a/src/app/api/llamaUtils.ts b/src/app/api/llamaUtils.ts
index 9c70edf..c815d71 100644
--- a/src/app/api/llamaUtils.ts
+++ b/src/app/api/llamaUtils.ts
@@ -7,6 +7,7 @@ import { NextResponse } from 'next/server';
 import { getAllChatMessagesInOrder, getHostSessionById } from '@/lib/db';
 import { GEMINI_MODEL } from 'llamaindex';
 import { initializeCrossPollination } from '@/lib/crossPollination';
+import { getLLM } from '@/lib/modelConfig';
 
 const basicFacilitationPrompt = `You are a skilled facilitator helping guide productive discussions. Your role is to:
 
@@ -39,30 +40,20 @@ export async function finishedResponse(
     userPrompt,
   });
 
-  const chatEngine = new Gemini({
-    model: GEMINI_MODEL.GEMINI_2_0_FLASH_THINKING_EXP,
-    temperature: 0.3,
-  });
+  const chatEngine = getLLM('MAIN', 0.3);
 
   try {
-    const messages: ChatMessage[] = [
-      {
-        role: 'system',
-        content: systemPrompt,
-      },
-      {
-        role: 'user',
-        content: userPrompt,
-      },
-    ];
-
-    console.log(
-      '[i] Generating completion with messages:',
-      JSON.stringify(messages, null, 2),
-    );
-
     const response = await chatEngine.chat({
-      messages: messages,
+      messages: [
+        {
+          role: 'system',
+          content: systemPrompt,
+        },
+        {
+          role: 'user',
+          content: userPrompt,
+        },
+      ],
     });
 
     console.log('[i] Completion response:', response);
@@ -165,10 +156,7 @@ Session Information:
 ${sessionData?.context ? `- Background Context: ${sessionData.context}` : ''}
 ${sessionData?.critical ? `- Key Points: ${sessionData.critical}` : ''}`;
 
-  const chatEngine = new Gemini({
-    model: GEMINI_MODEL.GEMINI_2_0_FLASH_THINKING_EXP,
-    temperature: 0.3,
-  });
+  const chatEngine = getLLM('MAIN', 0.3);
 
   const formattedMessages = [
     { role: 'system', content: sessionContext },
@@ -232,30 +220,20 @@ function streamResponse(systemPrompt: string, userPrompt: string) {
 
   return new ReadableStream({
     async start(controller) {
-      const chatEngine = new Gemini({
-        model: GEMINI_MODEL.GEMINI_2_0_FLASH_THINKING_EXP,
-        temperature: 0.3,
-      });
+      const chatEngine = getLLM('MAIN', 0.3);
 
       try {
-        const messages: ChatMessage[] = [
-          {
-            role: 'system',
-            content: systemPrompt,
-          },
-          {
-            role: 'user',
-            content: userPrompt,
-          },
-        ];
-
-        console.log(
-          '[i] Starting stream with messages:',
-          JSON.stringify(messages, null, 2),
-        );
-
         const response = await chatEngine.chat({
-          messages: messages,
+          messages: [
+            {
+              role: 'system',
+              content: systemPrompt,
+            },
+            {
+              role: 'user',
+              content: userPrompt,
+            },
+          ],
         });
 
         if (response.message?.content) {
diff --git a/src/lib/characterGenerator.ts b/src/lib/characterGenerator.ts
index ebe3875..60b70a6 100644
--- a/src/lib/characterGenerator.ts
+++ b/src/lib/characterGenerator.ts
@@ -1,4 +1,4 @@
-import { OpenAI as LlamaOpenAI } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 import * as db from '@/lib/db';
 
 export async function generateCharacters(sessionId: string): Promise<string> {
@@ -15,13 +15,8 @@ export async function generateCharacters(sessionId: string): Promise<string> {
     throw new Error('Session data not found');
   }
 
-  // Set up LlamaIndex chat model
-  const llm = new LlamaOpenAI({
-    model: 'gpt-4o-mini',
-    apiKey: process.env.OPENAI_API_KEY,
-    maxTokens: 1500,
-    temperature: 0.8,
-  });
+  // Set up LLM with SMALL model configuration
+  const llm = getLLM('SMALL', 0.8);
 
   const prompt = `Given this workshop context:
 Topic: ${sessionData.topic}
diff --git a/src/lib/crossPollinationManager.ts b/src/lib/crossPollinationManager.ts
index df18cf3..4023654 100644
--- a/src/lib/crossPollinationManager.ts
+++ b/src/lib/crossPollinationManager.ts
@@ -1,5 +1,4 @@
-import { Gemini } from 'llamaindex';
-import { GEMINI_MODEL } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 import { Message } from './schema';
 import {
   getAllChatMessagesInOrder,
@@ -23,17 +22,17 @@ export interface IdeaCluster {
 }
 
 export class CrossPollinationManager {
-  private chatEngine: Gemini;
+  private analyzeEngine;
+  private generateEngine;
   private config: CrossPollinationConfig;
   private lastCrossPollination: number | null = null;
   private sessionData: any = null;
 
   constructor(config: CrossPollinationConfig) {
     this.config = config;
-    this.chatEngine = new Gemini({
-      model: GEMINI_MODEL.GEMINI_2_0_FLASH_THINKING_EXP,
-      temperature: 0.3,
-    });
+    // Initialize with specific models for each purpose
+    this.analyzeEngine = getLLM('MAIN', 0.3);
+    this.generateEngine = getLLM('LARGE', 0.3);
   }
 
   async loadSessionData(): Promise<any> {
@@ -80,7 +79,7 @@ export class CrossPollinationManager {
       }
 
       // 4. Analyze the current thread to determine if cross-pollination is appropriate
-      const response = await this.chatEngine.chat({
+      const response = await this.analyzeEngine.chat({
         messages: [
           {
             role: 'system',
@@ -202,7 +201,7 @@ Based on this information, should I introduce cross-pollination now? Answer with
       );
 
       // Make a single LLM call to analyze and generate a question
-      const response = await this.chatEngine.chat({
+      const response = await this.generateEngine.chat({
         messages: [
           {
             role: 'system',
diff --git a/src/lib/export/llamaExport.ts b/src/lib/export/llamaExport.ts
index 02ae0f3..ef5e8f6 100644
--- a/src/lib/export/llamaExport.ts
+++ b/src/lib/export/llamaExport.ts
@@ -1,60 +1,64 @@
-import { Gemini, GEMINI_MODEL } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 import { Message, UserSession } from '@/lib/schema';
 
 /**
- * Extracts structured data from user messages using LlamaIndex's Gemini model
+ * Extracts structured data from user messages using configured LLM
  * @param context Array of conversation strings to analyze
  * @param exportInstructions Instructions for how to extract and format the data
  * @returns Formatted string with structured data (typically JSON)
  */
 export async function extractDataWithLlama(
   context: string[],
-  exportInstructions: string
+  exportInstructions: string,
 ): Promise<string> {
   try {
-    // Initialize Gemini model
-    const chatEngine = new Gemini({
-      model: GEMINI_MODEL.GEMINI_PRO_LATEST,
-      temperature: 0.3, // Low temperature for more deterministic structured output
-    });
-    
+    // Initialize LLM with MAIN model (using lower temperature for structured output)
+    const chatEngine = getLLM('LARGE', 0.3);
+
     // Format context and instructions
-    const formattedContext = '--- START USER CONVERSATION: ---\n\n' + context.join('\n\n--- NEXT USER CONVERSATION: ---\n\n');
-    console.log(`[llamaExport] Processing ${context.length} conversations for structured data extraction`);
-    
+    const formattedContext =
+      '--- START USER CONVERSATION: ---\n\n' +
+      context.join('\n\n--- NEXT USER CONVERSATION: ---\n\n');
+    console.log(
+      `[llamaExport] Processing ${context.length} conversations for structured data extraction`,
+    );
+
     // Get the response
     const response = await chatEngine.chat({
       messages: [
-        { 
-          role: 'system', 
-          content: `You are a data extraction assistant that formats conversation data according to specific instructions. Always return valid JSON without markdown formatting or code blocks.` 
+        {
+          role: 'system',
+          content: `You are a data extraction assistant that formats conversation data according to specific instructions. Always return valid JSON without markdown formatting or code blocks.`,
         },
         {
           role: 'user',
-          content: exportInstructions
+          content: exportInstructions,
         },
         {
           role: 'user',
-          content: `Here are the conversations to extract data from:\n\n${formattedContext}\n\nIMPORTANT: Return ONLY the JSON without any backticks, markdown formatting, or explanations.`
-        }
+          content: `Here are the conversations to extract data from:\n\n${formattedContext}\n\nIMPORTANT: Return ONLY the JSON without any backticks, markdown formatting, or explanations.`,
+        },
       ],
     });
-    
+
     // Clean the response if it includes markdown code blocks
     let content = response.message.content.toString();
-    
+
     // Remove markdown code blocks if present (```json ... ```)
-    if (content.includes("```")) {
+    if (content.includes('```')) {
       // Extract content between code block markers
       const codeBlockMatch = content.match(/```(?:json)?\s*([\s\S]*?)```/);
       if (codeBlockMatch && codeBlockMatch[1]) {
         content = codeBlockMatch[1].trim();
       } else {
         // If regex failed but we know there are backticks, try simpler approach
-        content = content.replace(/```(?:json)?/g, "").replace(/```/g, "").trim();
+        content = content
+          .replace(/```(?:json)?/g, '')
+          .replace(/```/g, '')
+          .trim();
       }
     }
-    
+
     return content;
   } catch (error) {
     console.error('[llamaExport] LlamaIndex error:', error);
@@ -68,17 +72,23 @@ export async function extractDataWithLlama(
  * @param allMessages All messages from the user sessions
  * @returns Array of concatenated conversation strings
  */
-export function formatMessagesForExport(userData: UserSession[], allMessages: Message[]): string[] {
+export function formatMessagesForExport(
+  userData: UserSession[],
+  allMessages: Message[],
+): string[] {
   // Group messages by thread
-  const messagesByThread = allMessages.reduce((acc, message) => {
-    acc[message.thread_id] = acc[message.thread_id] || [];
-    acc[message.thread_id].push(message);
-    return acc;
-  }, {} as Record<string, Message[]>);
-  
+  const messagesByThread = allMessages.reduce(
+    (acc, message) => {
+      acc[message.thread_id] = acc[message.thread_id] || [];
+      acc[message.thread_id].push(message);
+      return acc;
+    },
+    {} as Record<string, Message[]>,
+  );
+
   // Format each thread as a conversation string
-  return Object.entries(messagesByThread).map(
-    ([threadId, messages]) => concatenateMessages(messages)
+  return Object.entries(messagesByThread).map(([threadId, messages]) =>
+    concatenateMessages(messages),
   );
 }
 
@@ -89,9 +99,9 @@ export function formatMessagesForExport(userData: UserSession[], allMessages: Me
  */
 function concatenateMessages(messagesFromOneUser: Message[]): string {
   messagesFromOneUser.sort(
-    (a, b) => a.created_at.getTime() - b.created_at.getTime()
+    (a, b) => a.created_at.getTime() - b.created_at.getTime(),
   );
   return messagesFromOneUser
     .map((message) => `${message.role} : ${message.content}`)
     .join('\n');
-}
\ No newline at end of file
+}
diff --git a/src/lib/formAnswerGenerator.ts b/src/lib/formAnswerGenerator.ts
index afac647..3d6d046 100644
--- a/src/lib/formAnswerGenerator.ts
+++ b/src/lib/formAnswerGenerator.ts
@@ -1,4 +1,4 @@
-import { OpenAI as LlamaOpenAI } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 
 interface FormQuestion {
   id: string;
@@ -14,11 +14,7 @@ export async function generateFormAnswers(
   userContext: Record<string, string>,
   prompt?: string,
 ): Promise<string> {
-  const llm = new LlamaOpenAI({
-    model: 'gpt-4o-mini',
-    apiKey: process.env.OPENAI_API_KEY,
-    temperature: 0.5,
-  });
+  const llm = getLLM('SMALL', 0.5);
 
   const answers: Record<string, string> = {};
   console.log('[i] prompt', prompt);
diff --git a/src/lib/modelConfig.ts b/src/lib/modelConfig.ts
new file mode 100644
index 0000000..1648a75
--- /dev/null
+++ b/src/lib/modelConfig.ts
@@ -0,0 +1,65 @@
+import { OpenAI as LlamaOpenAI, Gemini, GEMINI_MODEL } from 'llamaindex';
+
+type Provider = 'openai' | 'anthropic' | 'gemini';
+type LLMInstance = LlamaOpenAI | Gemini;
+
+interface BaseModelConfig {
+  model: string;
+  provider: Provider;
+  apiKey: string;
+}
+
+const getApiKey = (provider: Provider): string => {
+  switch (provider) {
+    case 'openai':
+      return process.env.OPENAI_API_KEY || '';
+    case 'anthropic':
+      return process.env.ANTHROPIC_API_KEY || '';
+    case 'gemini':
+      return process.env.GOOGLE_API_KEY || '';
+    default:
+      throw new Error(`Unsupported provider: ${provider}`);
+  }
+};
+
+const getGeminiModel = (modelName: string) => {
+  const geminiModels = Object.values(GEMINI_MODEL);
+  const model = geminiModels.find((m) => m === modelName);
+  if (!model) {
+    throw new Error(
+      `Invalid Gemini model: ${modelName}. Must be one of: ${geminiModels.join(', ')}`,
+    );
+  }
+  return model;
+};
+
+export const getLLM = (
+  type: 'SMALL' | 'MAIN' | 'LARGE',
+  temperature = 0.7,
+): LLMInstance => {
+  const model = process.env[`${type}_LLM_MODEL`];
+  const provider = process.env[`${type}_LLM_PROVIDER`] as Provider;
+
+  if (!model || !provider) {
+    throw new Error(`Missing configuration for ${type}_LLM`);
+  }
+
+  const apiKey = getApiKey(provider);
+
+  switch (provider) {
+    case 'openai':
+      return new LlamaOpenAI({
+        model,
+        apiKey,
+        maxTokens: 150,
+        temperature,
+      });
+    case 'gemini':
+      return new Gemini({
+        model: getGeminiModel(model),
+        temperature,
+      });
+    default:
+      throw new Error(`Provider ${provider} not implemented`);
+  }
+};
diff --git a/src/lib/monica/analyzeQueryType.ts b/src/lib/monica/analyzeQueryType.ts
index e3c5435..cd85bec 100644
--- a/src/lib/monica/analyzeQueryType.ts
+++ b/src/lib/monica/analyzeQueryType.ts
@@ -1,4 +1,4 @@
-import { OpenAI as LlamaOpenAI } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 
 export type QueryClassification = {
   type: 'analytical' | 'specific';
@@ -8,11 +8,7 @@ export type QueryClassification = {
 export async function analyzeQueryType(
   query: string,
 ): Promise<QueryClassification> {
-  const classificationModel = new LlamaOpenAI({
-    model: 'gpt-3.5-turbo', // Using smaller model for classification
-    temperature: 0, // Zero temperature for consistent results
-    apiKey: process.env.OPENAI_API_KEY,
-  });
+  const llm = getLLM('MAIN', 0); // Using zero temperature for consistent results
 
   const classificationPrompt = `Analyze if this question requires RAG (specific) or analytical processing: "${query}"
 
@@ -39,12 +35,17 @@ export async function analyzeQueryType(
     Return only a JSON object: {"type":"analytical"|"specific","confidence":0.0-1.0}`;
 
   try {
-    const classificationResponse = await classificationModel.complete({
-      prompt: classificationPrompt,
+    const response = await llm.chat({
+      messages: [
+        {
+          role: 'user',
+          content: classificationPrompt,
+        },
+      ],
     });
 
     const queryClassification = JSON.parse(
-      classificationResponse.text as string,
+      response.message.content.toString(),
     ) as QueryClassification;
     return queryClassification;
   } catch (error) {
diff --git a/src/lib/participantAnswerGenerator.ts b/src/lib/participantAnswerGenerator.ts
index 696e278..01200e9 100644
--- a/src/lib/participantAnswerGenerator.ts
+++ b/src/lib/participantAnswerGenerator.ts
@@ -1,16 +1,10 @@
-import { OpenAI as LlamaOpenAI } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 import { getAllChatMessagesInOrder } from '@/lib/db';
 import { NewMessage } from '@/lib/schema';
 
-enum ModelProvider {
-  GPT4_MINI = 'gpt-4o-mini',
-  GPT3_TURBO = 'gpt-3.5-turbo',
-}
-
 interface AnswerGeneratorConfig {
   threadId: string;
   temperature?: number;
-  modelProvider?: ModelProvider;
 }
 
 const TEAM_MEMBER_PROMPT = `You are a creative UI/UX lead participating in this discussion. Follow these guidelines:
@@ -41,12 +35,10 @@ export async function generateParticipantAnswer(
   config: AnswerGeneratorConfig,
 ): Promise<NewMessage> {
   try {
-    const {
-      threadId,
-      temperature = 0.7,
-      modelProvider = ModelProvider.GPT4_MINI,
-    } = config;
+    const { threadId, temperature = 0.7 } = config;
 
+    // Initialize LLM with SMALL model configuration
+    const llm = getLLM('SMALL', temperature);
     console.log(`[i] Generating participant answer for thread: ${threadId}`);
 
     // Get all messages in the thread
@@ -75,17 +67,7 @@ export async function generateParticipantAnswer(
       )
       .join('\n\n');
 
-    // Set up OpenAI client
-    const llm = new LlamaOpenAI({
-      model: modelProvider,
-      apiKey: process.env.OPENAI_API_KEY,
-      maxTokens: 150, // Reduced token count for shorter responses
-      temperature: temperature,
-    });
-
-    console.log('[i] Generating participant response with OpenAI');
-
-    // Generate response
+    // Generate response using configured LLM
     const response = await llm.chat({
       messages: [
         {
diff --git a/src/lib/sessionGenerator.ts b/src/lib/sessionGenerator.ts
index a240d98..c386fad 100644
--- a/src/lib/sessionGenerator.ts
+++ b/src/lib/sessionGenerator.ts
@@ -1,21 +1,14 @@
-import { OpenAI as LlamaOpenAI } from 'llamaindex';
 import * as db from '@/lib/db';
 import * as llama from '../app/api/llamaUtils';
 import { getUserNameFromContext } from '@/lib/clientUtils';
 import { generateFormAnswers } from './formAnswerGenerator';
-
-enum ModelProvider {
-  GPT4 = 'gpt-4o-mini',
-  GPT3 = 'gpt-3.5-turbo',
-  CLAUDE = 'claude-3-sonnet',
-}
+import { getLLM } from '@/lib/modelConfig';
 
 interface SessionConfig {
   maxTurns: number;
   sessionId: string;
   temperature?: number;
   responsePrompt?: string;
-  modelProvider?: ModelProvider;
 }
 
 const DEFAULT_RESPONSE_PROMPT = `You are simulating user responses. Follow these guidelines:
@@ -28,12 +21,7 @@ async function isSessionComplete(
   lastQuestion: string,
   sessionData: any,
 ): Promise<boolean> {
-  const llm = new LlamaOpenAI({
-    model: 'gpt-4o-mini',
-    apiKey: process.env.OPENAI_API_KEY,
-    maxTokens: 100,
-    temperature: 0.1, // Low temperature for more consistent results
-  });
+  const llm = getLLM('MAIN', 0.1); // Low temperature for more consistent results
 
   const prompt = `Given this workshop context:
 Topic: ${sessionData.topic}
@@ -60,11 +48,8 @@ Reply with ONLY "true" if the session should end, or "false" if it should contin
 
 export async function generateSession(config: SessionConfig) {
   try {
-    const {
-      temperature = 0.7,
-      responsePrompt = DEFAULT_RESPONSE_PROMPT,
-      modelProvider = ModelProvider.GPT4,
-    } = config;
+    const { temperature = 0.7, responsePrompt = DEFAULT_RESPONSE_PROMPT } =
+      config;
 
     // Get session data from DB
     const sessionData = await db.getFromHostSession(config.sessionId, [
@@ -101,13 +86,8 @@ export async function generateSession(config: SessionConfig) {
       );
     }
 
-    // Set up LlamaIndex chat model
-    const llm = new LlamaOpenAI({
-      model: modelProvider,
-      apiKey: process.env.OPENAI_API_KEY,
-      maxTokens: 1000,
-      temperature: temperature,
-    });
+    // Set up LLM with MAIN model configuration
+    const llm = getLLM('MAIN', temperature);
 
     const threadId = await createThreadWithContext(config, userContextPrompt);
     let turnCount = 0;
@@ -147,7 +127,7 @@ export async function generateSession(config: SessionConfig) {
         break;
       }
 
-      // Generate response using LlamaIndex OpenAI
+      // Generate response using LLM
       const userResponse = await llm.chat({
         messages: [
           {
@@ -165,9 +145,6 @@ Additional response guidelines:
    - Brief references to background
 5. Use natural language with occasional filler words or expressions
 6. IMPORTANT: Do not use or make up specific names - always refer to people by their roles or relationships instead
-   - Even if names are provided in the context, replace them with roles (e.g., "I" or "my colleague" instead of "Clara")
-   - Never create new names or pseudonyms
-   - Use professional roles, relationships, or first-person perspective instead
 7. Include a mix of response types:
    - Sometimes disagree or express skepticism
    - Occasionally share negative experiences or concerns
@@ -225,26 +202,6 @@ function isConversationComplete(message: string): boolean {
   );
 }
 
-async function generateUserResponse(
-  client: LlamaOpenAI,
-  params: {
-    threadId: string;
-    question: string;
-    context?: Record<string, string>;
-    turnCount: number;
-  },
-): Promise<string> {
-  const prompt = `Given this context: ${JSON.stringify(params.context)}
-    And this question: "${params.question}"
-    Generate a realistic user response for turn ${params.turnCount + 1}.`;
-
-  const response = await client.chat({
-    messages: [{ role: 'user', content: prompt }],
-  });
-
-  return response.message.content?.toString() || ''; // Convert to string
-}
-
 async function createThreadWithContext(config: SessionConfig, context: string) {
   // Format user context as entry message
   const userContextPrompt = context
diff --git a/src/lib/summaryMultiSession.ts b/src/lib/summaryMultiSession.ts
index 5c9f6eb..dfeea19 100644
--- a/src/lib/summaryMultiSession.ts
+++ b/src/lib/summaryMultiSession.ts
@@ -1,7 +1,5 @@
-import { GEMINI_MODEL } from 'llamaindex';
-
 import * as db from '@/lib/db';
-import { Gemini } from 'llamaindex';
+import { getLLM } from '@/lib/modelConfig';
 
 const initialPrompt = `You are an expert in synthesizing conversations and creating insightful summaries. Your task is to analyze the session and create a clear, actionable summary.
 
@@ -161,10 +159,7 @@ ${sessionsData[sessionIndex]?.critical ? `Key Points: ${sessionsData[sessionInde
         .replace(/\b(we|our|ours)\b/gi, `Group${sessionNum}`);
     }
 
-    const chatEngine = new Gemini({
-      model: GEMINI_MODEL.GEMINI_PRO_LATEST,
-      temperature: 0.3,
-    });
+    const chatEngine = getLLM('MAIN', 0.3);
 
     const userPrompt = `
 ### Historical Messages by Session:
